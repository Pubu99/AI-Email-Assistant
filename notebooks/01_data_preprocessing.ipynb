{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbee6e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¥ Loaded 517401 emails.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 517401/517401 [00:41<00:00, 12443.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Using 85110 emails after filtering and sampling.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85110/85110 [00:01<00:00, 81713.95it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85110/85110 [1:07:23<00:00, 21.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Cleaned data saved to: data/processed/clean_emails.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_body</th>\n",
       "      <th>entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>giron-d/_sent_mail/576.</td>\n",
       "      <td>Data Request</td>\n",
       "      <td>---------------------- Forwarded by Darron C G...</td>\n",
       "      <td>{'PERSON': [], 'DATE': [], 'ORG': ['Enron Nort...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>watson-k/e_mail_bin/562.</td>\n",
       "      <td>General Inquiry</td>\n",
       "      <td>RIGZONE DAILY NEWS -- TUESDAY, MARCH 12, 2002 ...</td>\n",
       "      <td>{'PERSON': [], 'DATE': ['MARCH 12, 2002', 'TUE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>keavey-p/all_documents/409.</td>\n",
       "      <td>Meeting Request</td>\n",
       "      <td>The information contained herein is based on s...</td>\n",
       "      <td>{'PERSON': ['Carr Futures'], 'DATE': ['2001'],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lokay-m/all_documents/633.</td>\n",
       "      <td>Job Inquiry</td>\n",
       "      <td>Commercialize your intelligence on the Edge th...</td>\n",
       "      <td>{'PERSON': ['Cindy Olson's', 'Steve Hotte', 'S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>meyers-a/deleted_items/1113.</td>\n",
       "      <td>Finance</td>\n",
       "      <td>To Whom this may concern: Please note that Col...</td>\n",
       "      <td>{'PERSON': ['Bert Meyers'], 'DATE': ['711291',...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           file            label  \\\n",
       "0       giron-d/_sent_mail/576.     Data Request   \n",
       "1      watson-k/e_mail_bin/562.  General Inquiry   \n",
       "2   keavey-p/all_documents/409.  Meeting Request   \n",
       "3    lokay-m/all_documents/633.      Job Inquiry   \n",
       "4  meyers-a/deleted_items/1113.          Finance   \n",
       "\n",
       "                                          clean_body  \\\n",
       "0  ---------------------- Forwarded by Darron C G...   \n",
       "1  RIGZONE DAILY NEWS -- TUESDAY, MARCH 12, 2002 ...   \n",
       "2  The information contained herein is based on s...   \n",
       "3  Commercialize your intelligence on the Edge th...   \n",
       "4  To Whom this may concern: Please note that Col...   \n",
       "\n",
       "                                            entities  \n",
       "0  {'PERSON': [], 'DATE': [], 'ORG': ['Enron Nort...  \n",
       "1  {'PERSON': [], 'DATE': ['MARCH 12, 2002', 'TUE...  \n",
       "2  {'PERSON': ['Carr Futures'], 'DATE': ['2001'],...  \n",
       "3  {'PERSON': ['Cindy Olson's', 'Steve Hotte', 'S...  \n",
       "4  {'PERSON': ['Bert Meyers'], 'DATE': ['711291',...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === 01_data_preprocessing.ipynb ===\n",
    "\n",
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Enable tqdm for pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"../data/raw/emails.csv\")\n",
    "df = df[[\"file\", \"message\"]].dropna()\n",
    "print(f\"ðŸ“¥ Loaded {len(df)} emails.\")\n",
    "\n",
    "# Extract main email body (remove headers & long quotes)\n",
    "def extract_body_from_message(message):\n",
    "    try:\n",
    "        body = re.split(r'\\n\\s*\\n', message, maxsplit=1)[-1]  # remove headers\n",
    "        body = re.sub(r'\\s+', ' ', body)  # normalize spaces\n",
    "        body = re.split(r'(-----Original Message-----|_+Forwarded by|From:.*@)', body)[0]  # cut long reply chains\n",
    "        return body.strip()\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "df[\"clean_body\"] = df[\"message\"].progress_apply(extract_body_from_message)\n",
    "\n",
    "# Remove very long or very short messages\n",
    "df = df[df[\"clean_body\"].str.len() > 100]        # too short = useless\n",
    "df = df[df[\"clean_body\"].str.len() < 100000]     # too long = spaCy crash\n",
    "\n",
    "# (Removed sampling step to use full dataset)\n",
    "\n",
    "# Heuristic intent labeling using keywords\n",
    "INTENT_KEYWORDS = {\n",
    "    \"Meeting Request\": [\"schedule\", \"meeting\", \"calendar\", \"call\", \"appointment\"],\n",
    "    \"Job Inquiry\": [\"job\", \"resume\", \"position\", \"career\", \"apply\"],\n",
    "    \"Finance\": [\"invoice\", \"payment\", \"amount\", \"fund\", \"salary\", \"finance\"],\n",
    "    \"Legal\": [\"contract\", \"agreement\", \"terms\", \"clause\", \"lawyer\"],\n",
    "    \"Appreciation\": [\"thank\", \"thanks\", \"grateful\", \"appreciate\", \"gratitude\"],\n",
    "    \"Complaint\": [\"issue\", \"problem\", \"complaint\", \"error\", \"concern\"],\n",
    "    \"Technical Support\": [\"bug\", \"support\", \"crash\", \"error\", \"fix\", \"install\"],\n",
    "    \"Data Request\": [\"send\", \"forward\", \"email\", \"attach\", \"request\"],\n",
    "    \"Greeting\": [\"hello\", \"hi\", \"greetings\", \"good morning\"],\n",
    "    \"Farewell\": [\"regards\", \"bye\", \"sincerely\", \"take care\"],\n",
    "    \"Sales Inquiry\": [\"quote\", \"pricing\", \"discount\", \"offer\", \"deal\"],\n",
    "    \"Project Update\": [\"progress\", \"update\", \"status\", \"report\"],\n",
    "    \"Reminder\": [\"remind\", \"deadline\", \"follow up\", \"due\"],\n",
    "    \"Event Planning\": [\"venue\", \"event\", \"conference\", \"webinar\", \"party\"],\n",
    "    \"Personal\": [\"family\", \"friend\", \"wedding\", \"vacation\", \"holiday\"]\n",
    "}\n",
    "\n",
    "def assign_label(text):\n",
    "    text = text.lower()\n",
    "    for label, keywords in INTENT_KEYWORDS.items():\n",
    "        if any(kw in text for kw in keywords):\n",
    "            return label\n",
    "    return \"General Inquiry\"\n",
    "\n",
    "df[\"label\"] = df[\"clean_body\"].progress_apply(assign_label)\n",
    "\n",
    "# Load spaCy model and set max length\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.max_length = 300000  # increase limit to avoid crash on long text\n",
    "\n",
    "# ðŸ”Ž Extract named entities: PERSON, DATE, ORG, GPE\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    return {\n",
    "        \"PERSON\": list(set(ent.text for ent in doc.ents if ent.label_ == \"PERSON\")),\n",
    "        \"DATE\": list(set(ent.text for ent in doc.ents if ent.label_ == \"DATE\")),\n",
    "        \"ORG\": list(set(ent.text for ent in doc.ents if ent.label_ == \"ORG\")),\n",
    "        \"GPE\": list(set(ent.text for ent in doc.ents if ent.label_ == \"GPE\"))\n",
    "    }\n",
    "\n",
    "df[\"entities\"] = df[\"clean_body\"].progress_apply(extract_entities)\n",
    "\n",
    "# Save cleaned dataset\n",
    "os.makedirs(\"../data/processed/\", exist_ok=True)\n",
    "df.to_csv(\"../data/processed/clean_emails.csv\", index=False)\n",
    "print(\"âœ… Cleaned data saved to: data/processed/clean_emails.csv\")\n",
    "\n",
    "# Preview\n",
    "df[[\"file\", \"label\", \"clean_body\", \"entities\"]].head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
