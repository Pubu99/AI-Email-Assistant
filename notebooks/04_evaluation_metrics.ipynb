{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2032e7fa",
   "metadata": {},
   "source": [
    "# Comprehensive Model Evaluation\n",
    "\n",
    "This notebook provides a detailed evaluation of our email assistant models including:\n",
    "1. Intent Classification Metrics\n",
    "2. Reply Generation Quality\n",
    "3. System Performance Metrics\n",
    "4. A/B Testing Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a94e8dd",
   "metadata": {},
   "source": [
    "### Imports and Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddb308e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Email Genrator\\AI-Email-Assistant\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import pickle\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "import json\n",
    "import tqdm\n",
    "\n",
    "tqdm.tqdm.pandas()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33151dc",
   "metadata": {},
   "source": [
    "### Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38478499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data with labels and targets\n",
    "df = pd.read_csv(\"../data/processed/clean_emails.csv\")\n",
    "df = df[[\"clean_body\", \"label\", \"entities\"]].dropna()\n",
    "\n",
    "# Parse entities\n",
    "def parse_entities(ent_str):\n",
    "    try:\n",
    "        return {k: v for k, v in json.loads(ent_str.replace(\"'\", '\"')).items() if v}\n",
    "    except:\n",
    "        return {}\n",
    "\n",
    "df[\"parsed_entities\"] = df[\"entities\"].apply(parse_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2bb673",
   "metadata": {},
   "source": [
    "### Evaluate Intent Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d678854a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 425552/425552 [3:48:15<00:00, 31.07it/s]  \n",
      "d:\\Email Genrator\\AI-Email-Assistant\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "d:\\Email Genrator\\AI-Email-Assistant\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Intent Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "     Appreciation       0.95      0.97      0.96     60567\n",
      "        Complaint       0.90      0.87      0.89     13994\n",
      "     Data Request       0.93      0.97      0.95     57807\n",
      "   Event Planning       0.00      0.00      0.00       904\n",
      "         Farewell       0.00      0.00      0.00       884\n",
      "          Finance       0.84      0.84      0.84     17805\n",
      "  General Inquiry       0.74      0.93      0.82     18796\n",
      "         Greeting       0.85      0.88      0.86     32509\n",
      "      Job Inquiry       0.90      0.78      0.84     20744\n",
      "            Legal       0.92      0.93      0.92     29772\n",
      "  Meeting Request       0.94      0.93      0.93    162634\n",
      "         Personal       0.00      0.00      0.00       663\n",
      "   Project Update       0.97      0.08      0.16      1256\n",
      "         Reminder       0.00      0.00      0.00       571\n",
      "    Sales Inquiry       0.82      0.69      0.75      2372\n",
      "Technical Support       0.81      0.74      0.77      4274\n",
      "\n",
      "         accuracy                           0.91    425552\n",
      "        macro avg       0.66      0.60      0.61    425552\n",
      "     weighted avg       0.91      0.91      0.91    425552\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Email Genrator\\AI-Email-Assistant\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "# Load model + tokenizer\n",
    "intent_model_dir = \"../models/intent_classifier\"\n",
    "intent_tokenizer = DistilBertTokenizerFast.from_pretrained(intent_model_dir)\n",
    "intent_model = DistilBertForSequenceClassification.from_pretrained(intent_model_dir).to(device)\n",
    "\n",
    "with open(f\"{intent_model_dir}/label_encoder.pkl\", \"rb\") as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "\n",
    "def predict_intent(text):\n",
    "    inputs = intent_tokenizer(text, truncation=True, padding=True, return_tensors=\"pt\", max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = intent_model(**inputs).logits\n",
    "    pred = torch.argmax(logits, dim=1).item()\n",
    "    return label_encoder.inverse_transform([pred])[0]\n",
    "\n",
    "df[\"predicted_label\"] = df[\"clean_body\"].progress_apply(predict_intent)\n",
    "\n",
    "# Evaluate\n",
    "report = classification_report(df[\"label\"], df[\"predicted_label\"])\n",
    "print(\"ðŸ“Š Intent Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6fd404",
   "metadata": {},
   "source": [
    "### Evaluate Reply Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "523f00d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [02:33<00:00,  3.25it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [02:33<00:00,  3.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load T5 model\n",
    "reply_model_dir = \"../models/reply_generator\"\n",
    "reply_model = T5ForConditionalGeneration.from_pretrained(reply_model_dir).to(device)\n",
    "reply_tokenizer = T5Tokenizer.from_pretrained(reply_model_dir)\n",
    "\n",
    "# Prompt builder\n",
    "def build_prompt(row):\n",
    "    entities = row[\"parsed_entities\"]\n",
    "    recipient = entities.get(\"PERSON\", [\"Unknown\"])[0]\n",
    "    entities_str = \" | \".join(f\"{k}: {', '.join(v)}\" for k, v in entities.items()) if entities else \"None\"\n",
    "    return f\"Intent: {row['label']} | RecipientName: {recipient} | Entities: {entities_str} | Email: {row['clean_body']}\"\n",
    "\n",
    "# Sample a smaller subset for quick evaluation\n",
    "df_sample = df.sample(n=500, random_state=42).reset_index(drop=True)\n",
    "df_sample[\"prompt\"] = df_sample.apply(build_prompt, axis=1)\n",
    "df_sample[\"target\"] = df_sample[\"prompt\"]  # weâ€™ll generate reply from the prompt\n",
    "\n",
    "# Generate replies\n",
    "def generate_reply(prompt):\n",
    "    inputs = reply_tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = reply_model.generate(**inputs, max_length=128)\n",
    "    return reply_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "df_sample[\"generated_reply\"] = df_sample[\"prompt\"].progress_apply(generate_reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cff20f",
   "metadata": {},
   "source": [
    "### Compute BLEU & ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "635745f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“˜ Avg BLEU: 0.0012\n",
      "ðŸ“• Avg ROUGE-L: 0.0580\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "smoothie = SmoothingFunction().method4\n",
    "bleu_scores, rouge_l_scores = [], []\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
    "\n",
    "for _, row in df_sample.iterrows():\n",
    "    ref = row[\"target\"]\n",
    "    gen = row[\"generated_reply\"]\n",
    "\n",
    "    # BLEU\n",
    "    bleu = sentence_bleu([ref.split()], gen.split(), smoothing_function=smoothie)\n",
    "    bleu_scores.append(bleu)\n",
    "\n",
    "    # ROUGE-L\n",
    "    rouge = scorer.score(ref, gen)[\"rougeL\"].fmeasure\n",
    "    rouge_l_scores.append(rouge)\n",
    "\n",
    "print(f\"ðŸ“˜ Avg BLEU: {sum(bleu_scores)/len(bleu_scores):.4f}\")\n",
    "print(f\"ðŸ“• Avg ROUGE-L: {sum(rouge_l_scores)/len(rouge_l_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d255559",
   "metadata": {},
   "source": [
    "### Save Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c68f4d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Evaluation results saved to ../data/processed/evaluation_output.csv\n"
     ]
    }
   ],
   "source": [
    "df_sample[[\"clean_body\", \"label\", \"prompt\", \"generated_reply\"]].to_csv(\"../data/processed/evaluation_output.csv\", index=False)\n",
    "print(\"âœ… Evaluation results saved to ../data/processed/evaluation_output.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
