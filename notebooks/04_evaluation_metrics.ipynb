{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2032e7fa",
   "metadata": {},
   "source": [
    "# Comprehensive Model Evaluation\n",
    "\n",
    "This notebook provides a detailed evaluation of our email assistant models including:\n",
    "1. Intent Classification Metrics\n",
    "2. Reply Generation Quality\n",
    "3. System Performance Metrics\n",
    "4. A/B Testing Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a94e8dd",
   "metadata": {},
   "source": [
    "### Imports and Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddb308e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Email Genrator\\AI-Email-Assistant\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import pickle\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "import json\n",
    "import tqdm\n",
    "\n",
    "tqdm.tqdm.pandas()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33151dc",
   "metadata": {},
   "source": [
    "### Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38478499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data with labels and targets\n",
    "df = pd.read_csv(\"../data/processed/clean_emails.csv\")\n",
    "df = df[[\"clean_body\", \"label\", \"entities\"]].dropna()\n",
    "\n",
    "# Parse entities\n",
    "def parse_entities(ent_str):\n",
    "    try:\n",
    "        return {k: v for k, v in json.loads(ent_str.replace(\"'\", '\"')).items() if v}\n",
    "    except:\n",
    "        return {}\n",
    "\n",
    "df[\"parsed_entities\"] = df[\"entities\"].apply(parse_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2bb673",
   "metadata": {},
   "source": [
    "### Evaluate Intent Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d678854a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model + tokenizer\n",
    "intent_model_dir = \"../models/intent_classifier\"\n",
    "intent_tokenizer = DistilBertTokenizerFast.from_pretrained(intent_model_dir)\n",
    "intent_model = DistilBertForSequenceClassification.from_pretrained(intent_model_dir).to(device)\n",
    "\n",
    "with open(f\"{intent_model_dir}/label_encoder.pkl\", \"rb\") as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "\n",
    "def predict_intent(text):\n",
    "    inputs = intent_tokenizer(text, truncation=True, padding=True, return_tensors=\"pt\", max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = intent_model(**inputs).logits\n",
    "    pred = torch.argmax(logits, dim=1).item()\n",
    "    return label_encoder.inverse_transform([pred])[0]\n",
    "\n",
    "df[\"predicted_label\"] = df[\"clean_body\"].progress_apply(predict_intent)\n",
    "\n",
    "# Evaluate\n",
    "report = classification_report(df[\"label\"], df[\"predicted_label\"])\n",
    "print(\"ðŸ“Š Intent Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6fd404",
   "metadata": {},
   "source": [
    "### Evaluate Reply Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523f00d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load T5 model\n",
    "reply_model_dir = \"../models/reply_generator\"\n",
    "reply_model = T5ForConditionalGeneration.from_pretrained(reply_model_dir).to(device)\n",
    "reply_tokenizer = T5Tokenizer.from_pretrained(reply_model_dir)\n",
    "\n",
    "# Prompt builder\n",
    "def build_prompt(row):\n",
    "    entities = row[\"parsed_entities\"]\n",
    "    recipient = entities.get(\"PERSON\", [\"Unknown\"])[0]\n",
    "    entities_str = \" | \".join(f\"{k}: {', '.join(v)}\" for k, v in entities.items()) if entities else \"None\"\n",
    "    return f\"Intent: {row['label']} | RecipientName: {recipient} | Entities: {entities_str} | Email: {row['clean_body']}\"\n",
    "\n",
    "df[\"prompt\"] = df.apply(build_prompt, axis=1)\n",
    "df[\"target\"] = df[\"prompt\"]  # weâ€™ll generate reply from the prompt\n",
    "\n",
    "# Generate replies\n",
    "def generate_reply(prompt):\n",
    "    inputs = reply_tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = reply_model.generate(**inputs, max_length=128)\n",
    "    return reply_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "df[\"generated_reply\"] = df[\"prompt\"].progress_apply(generate_reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cff20f",
   "metadata": {},
   "source": [
    "### Compute BLEU & ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635745f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "smoothie = SmoothingFunction().method4\n",
    "bleu_scores, rouge_l_scores = [], []\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    ref = row[\"target\"]\n",
    "    gen = row[\"generated_reply\"]\n",
    "\n",
    "    # BLEU\n",
    "    bleu = sentence_bleu([ref.split()], gen.split(), smoothing_function=smoothie)\n",
    "    bleu_scores.append(bleu)\n",
    "\n",
    "    # ROUGE-L\n",
    "    rouge = scorer.score(ref, gen)[\"rougeL\"].fmeasure\n",
    "    rouge_l_scores.append(rouge)\n",
    "\n",
    "print(f\"ðŸ“˜ Avg BLEU: {sum(bleu_scores)/len(bleu_scores):.4f}\")\n",
    "print(f\"ðŸ“• Avg ROUGE-L: {sum(rouge_l_scores)/len(rouge_l_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d255559",
   "metadata": {},
   "source": [
    "### Save Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68f4d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"clean_body\", \"label\", \"predicted_label\", \"prompt\", \"generated_reply\"]].to_csv(\"../data/processed/evaluation_output.csv\", index=False)\n",
    "print(\"âœ… Evaluation results saved to ../data/processed/evaluation_output.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
